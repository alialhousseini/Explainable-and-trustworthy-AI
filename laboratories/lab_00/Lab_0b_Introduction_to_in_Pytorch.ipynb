{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4_1MtQsGP9v"
   },
   "source": [
    "# **Lab 0 - Explainable and Trustworthy AI**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Teaching Assistant**: *Gabriele Ciravegna*\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "oaQT6wR2FQvW"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7N5PXdIGP9x"
   },
   "source": [
    "# **Lab 0 (2):** Introduction to Deep Learning in Pytorch\n",
    "PyTorch provides the elegantly designed modules and classes [torch.nn](https://pytorch.org/docs/stable/nn.html), [torch.optim](https://pytorch.org/docs/stable/optim.html), [Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset), and [DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)\n",
    "to help you create and train neural networks.\n",
    "\n",
    "In order to fully utilize their power and customize them for your problem, you need to really understand exactly what they're doing. To develop this understanding, we will first train basic neural net on the digits data set without using any features from these models; we will initially only use the most basic PyTorch tensor functionality. Then, we will incrementally add features from:\n",
    "1. ``torch.nn``\n",
    "2. ``torch.optim``\n",
    "3. ``Dataset``\n",
    "4. ``DataLoader``\n",
    "\n",
    "We will add one module at a time, showing exactly what each module does, and how it makes the code either more concise, or more flexible.\n",
    "\n",
    "**This tutorial assumes you already have PyTorch installed, and are familiar with the basics of tensor operations.** (If you're familiar with Numpy array operations, you'll find the PyTorch tensor operations used here nearly identical)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 0: Library loading and installation\n",
    "In case you don't have installed in your local computer the torch, numpy and scikit-learn packages, you can install them by running the following commands:\n",
    "```bash\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install scikit-learn\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "id": "L_OLTRgqFQvX"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install scikit-learn"
   ],
   "metadata": {
    "id": "qJ1q23I4GP9r"
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise1: Digits Prediction\n",
    "\n",
    "In this exercise we will train a simple logistic regression model to classify the digits in the  [**digits**](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) dataset, a simplified version of the [mnist](http://deeplearning.net/data/mnist/) dataset provided by scikit-learn.\n",
    "It consists of black-and-white images of hand-drawn digits (between 0 and 9).\n",
    "\n",
    "\n",
    "## Exercise 1.1: Data Loading and Visualization\n",
    "Similarly, to the previous lab, we will download it as a dataframe from scikit-learn.\n"
   ],
   "metadata": {
    "id": "Khe39_71P6re"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "### ENTER YOUR CODE HERE (3-4 lines expected)\n",
    "\n",
    "\n",
    "print(f\"Number of samples: {n_samples}, Number of features: {n_features}\")\n",
    "print(f\"Labels range {y.min()} - {y.max()}, Number of classes: {n_classes}\")\n"
   ],
   "metadata": {
    "id": "3c7ROCF9P6rf"
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    " Let's now visualize few statistics by using the ``describe`` method of the dataframe."
   ],
   "metadata": {
    "collapsed": false,
    "id": "uqkblg0oFQvY"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "### ENTER YOUR CODE HERE (1 line expected)\n"
   ],
   "metadata": {
    "id": "fbBFH7uXFQvY"
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do you already have any guess of which is the **least discriminative** feature?\n",
    "Also, shall we normalize the dataset?\n",
    "**ENTER YOUR ANSWER (few lines)**\n",
    "\n",
    "**END OF YOUR ANSWER**"
   ],
   "metadata": {
    "collapsed": false,
    "id": "gR7W8LucFQvY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each image is 8 x 8, and is being stored as a flattened row of length 64. Let's take a look at one; we need to reshape it to 2d first. Then we will use the ``imshow`` function from matplotlib to visualize it.\n"
   ],
   "metadata": {
    "id": "XYkMJ-X5P6rf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGk24RTlP6rg"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(x.iloc[3].values.reshape((8, 8)), cmap=\"gray\")\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpHEcVCkP6rf"
   },
   "source": [
    "## Exercise 1.2: Data Preprocessing\n",
    "\n",
    "We have to split the dataset into training and validation. To do so we can use one of the many functions provided by scikit-learn. For example we can call ``sklearn.model_selection.StratifiedKFold()`` to have a cross validation generator.\n",
    "\n",
    "For the sake of simplicity, however, in this tutorial we will use ``sklearn.model_selection.train_test_split`` which performs a stratified holdout cross-validation to create a single train test split (with 90% - 10% of data in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLZUs8MPP6rf"
   },
   "outputs": [],
   "source": [
    "### ENTER YOUR CODE HERE (2 lines expected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "I hope you positively answered the previous normalization question because yes, we **always need to normalize the data with neural networks**. The input data is stored with values in [0-16], we will normalize to [0-1].\n",
    "\n",
    "$x = \\frac{x - min(x)}{max(x)}$\n",
    "\n",
    "\n",
    "We will use scikit-learn to do so, with the ``MinMaxScaler`` class. Remember to fit the scaler only on the training data, and then apply it to both the training and validation data."
   ],
   "metadata": {
    "collapsed": false,
    "id": "T61jqd-wFQvZ"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "### ENTER YOUR CODE HERE (3 lines expected)\n",
    "\n",
    "\n",
    "print(f\"Train min: {x_train.min()}, max:{x_train.max()}\")\n",
    "print(f\"Test min: {x_test.min()}, max:{x_test.max()}\")"
   ],
   "metadata": {
    "id": "--beR92CFQvZ"
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "PyTorch uses ``torch.tensor``, rather than numpy arrays, so we need to\n",
    "convert our data.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "One7wQHPFQvZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFue-AfvP6rg"
   },
   "outputs": [],
   "source": [
    "x_train = torch.as_tensor(x_train, dtype=torch.float32)\n",
    "x_test = torch.as_tensor(x_test, dtype=torch.float32)\n",
    "y_train = torch.as_tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.as_tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qK3O1YQTGP92"
   },
   "source": [
    "## Exercise 1.3 Neural networks from scratch (no torch.nn)\n",
    "\n",
    "Let's first create a model using nothing but PyTorch tensor operations. We assume you're already familiar with the basics of neural networks.\n",
    "\n",
    "1. PyTorch provides methods to create random or zero-filled tensors, which we will use to create our weights and bias for a simple linear model.\n",
    "2. These are just regular tensors, with one very special addition: we tell PyTorch that they require a **gradient**. This causes PyTorch to record all the operations done on the tensor.\n",
    "3. This allows us and PyTorch to calculate the gradient during back-propagation **automatically**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Model\n",
    "\n",
    "We will use a very simple logistic regression with 10 output nodes (as many as the number of classes).\n",
    "\n",
    "For the weights, we set ``requires_grad`` after the initialization, since we\n",
    "don't want that step included in the gradient.\n",
    "\n",
    "##### To Note:\n",
    "1. We are initializing the weights here with the [Xavier initialisation](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), by multiplying with 1/sqrt(n_features)).\n",
    "2. The trailing ``_`` in PyTorch signifies that the operation is performed in-place (``requires_grad_()``)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "DnOoA2mycudn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nso__EchGP92"
   },
   "outputs": [],
   "source": [
    "# initialize weights and bias\n",
    "weights = torch.randn(n_features, n_classes)\n",
    "weights = weights / np.sqrt(n_features)\n",
    "bias = torch.randn(n_classes)\n",
    "bias = bias / np.sqrt(n_features)\n",
    "\n",
    "# tell pytorch it requires to compute the gradient\n",
    "weights.requires_grad_()\n",
    "bias.requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Thanks to PyTorch's ability to calculate gradients automatically, we can use any standard Python function (or callable object) as a model!\n",
    "\n",
    "Let's just write a plain **matrix multiplication** and **broadcasted addition** to create a simple **linear model**.\n",
    "\n",
    "We also need an activation function, we'll use a `log_softmax`.\n",
    "Although PyTorch provides lots of pre-written loss functions, activation functions, and so forth, you can easily write your own using plain python.\n",
    "PyTorch will even create fast GPU or vectorized CPU code for your function **automatically**."
   ],
   "metadata": {
    "collapsed": false,
    "id": "UR8d1KEFFQvZ"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias) # a @ b -> torch.matmul(a, b)\n",
    "\n",
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1) # => log(exp(x)/sum(exp(x_i)))\n"
   ],
   "metadata": {
    "id": "07VeegnRFQvZ"
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpSdCzfMGP93"
   },
   "source": [
    "In the above, the ``@`` stands for the *matrix multiplication* operation.\n",
    "\n",
    "Let's now call our model function on one batch of data (in this case, 64 images).  This is one *forward pass*.  \n",
    "\n",
    "\n",
    "Note: our predictions won't be any better than random at this stage, since we start with random weights and we have not done any training step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dY17VDFLGP94"
   },
   "outputs": [],
   "source": [
    "bs = 64               # batch size\n",
    "\n",
    "### ENTER YOUR CODE HERE (2 lines expected)\n",
    "# a mini-batch from x\n",
    "# predictions\n",
    "\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmJJ7kPoGP94"
   },
   "source": [
    "As you see, the ``preds`` tensor contains not only the tensor values, but also a gradient function. We'll use this later to do the backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss and Metric\n",
    "\n",
    "Let's implement negative log-likelihood to use as the loss function (again, we can just use standard Python).\n",
    "\n",
    "Note that the way in which we have implemented it is just a clever way of doing the selection of the prediction corresponding to the correct label as in: $\\sum_i y_i * log(f_i)$.\n",
    "\n",
    "You can find further information on how to compute the cross entropy loss in different scenarios in this [tutorial](https://towardsdatascience.com/cross-entropy-negative-log-likelihood-and-all-that-jazz-47a95bd2e81).\n",
    "\n",
    "\n",
    "If we check our loss with our random model now, we can see if we improve after a backprop pass later.\n",
    "\n"
   ],
   "metadata": {
    "id": "kZuGvyy7c-7K"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVhzufT0GP94"
   },
   "outputs": [],
   "source": [
    "def nll(output, target):\n",
    "    return -output[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll\n",
    "\n",
    "### ENTER YOUR CODE HERE (1 line expected)\n",
    "\n",
    "\n",
    "print(f\"Initial Loss: {l}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfXzwWbCGP95"
   },
   "source": [
    "Let's also implement a function to calculate the accuracy of our model.\n",
    "\n",
    "For each prediction, if the index with the largest value matches the target value, then the prediction was correct.\n",
    "\n",
    "Let's check the accuracy of our random model, so we can see if our accuracy improves as our loss improves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1AVMzXCGP95"
   },
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "\n",
    "    ### ENTER YOUR CODE HERE (2 lines expected)\n",
    "\n",
    "    return acc_val\n",
    "\n",
    "acc = accuracy(model(x_train), y_train)\n",
    "print(f\"Initial Accuracy: {acc:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOkrEDv2GP95"
   },
   "source": [
    "### The training loop\n",
    "\n",
    "We can now run a training loop.  For each iteration, we will:\n",
    "\n",
    "1. select a mini-batch of data (of size ``bs``)\n",
    "2. use the model to make predictions\n",
    "3. calculate the loss\n",
    "4. ``loss.backward()`` computes the gradients of the model, in this case, ``weights`` and ``bias``.\n",
    "5. update the parameters to optimize the model\n",
    "\n",
    "Note:\n",
    "\n",
    "- We do this within the ``torch.no_grad()`` context manager, because we do not want these actions to be recorded for our next calculation of the gradient.  You can read more about how PyTorch's Autograd records operations\n",
    "[here](https://pytorch.org/docs/stable/notes/autograd.html).\n",
    "- We then set the gradients to zero, so that we are ready for the next loop. Otherwise, our gradients would record a running tally of all the operations that had happened (i.e. ``loss.backward()`` *adds* the gradients to whatever is already stored, rather than replacing them).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gRahb4YGP96"
   },
   "outputs": [],
   "source": [
    "lr = .5  # learning rate\n",
    "epochs = 20  # how many epochs to train for\n",
    "train_samples = x_train.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((train_samples - 1) // bs + 1):\n",
    "\n",
    "        ## select input and label batch\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "\n",
    "        ## make predictions and compute loss\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        ## compute gradient through backprop\n",
    "        loss.backward()\n",
    "\n",
    "        ## update gradients through SGD\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "    print(f\"Epoch {epoch}: {loss_func(model(x_train), y_train):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwJKP_LTGP96"
   },
   "source": [
    "That's it: we've created and trained a minimal neural network (in this case, a logistic regression, since we have no hidden layers) entirely from scratch!\n",
    "\n",
    "Let's check the loss and accuracy and compare those to what we got earlier. We expect that the loss will have decreased and accuracy to have increased, and they have.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ETCDTnIGP96"
   },
   "outputs": [],
   "source": [
    "### ENTER YOUR CODE HERE (2 lines expected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqOgP0Q6GP96"
   },
   "source": [
    "#### Let's refactor our code using torch.nn\n",
    "\n",
    "We will now refactor our code, so that it does the same thing as before, only we'll start taking advantage of PyTorch's ``nn`` classes to make it more concise and flexible.\n",
    "\n",
    "At each step from here, we should be making our code one or more of:\n",
    "1. shorter\n",
    "2. more understandable\n",
    "3. more flexible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Exercise 1.4 Refactor using nn.functional\n",
    "The first and easiest step is to make our code shorter by replacing our hand-written activation and loss functions with those from ``torch.nn.functional`` (which is generally imported into the namespace ``F`` by convention).\n",
    "\n",
    "The ``torch.nn.functional`` module contains all the functions in the ``torch.nn`` library (whereas other parts of the library contain classes). As well as a wide range of loss and activation functions, you'll also find here some convenient functions for creating deep neural networks, such as pooling functions.\n",
    "\n",
    "If you're using negative log likelihood loss and log softmax activation, then Pytorch provides a single function ``F.cross_entropy`` that combines the two. So we can even remove the activation function from our model."
   ],
   "metadata": {
    "id": "OMZrCmLHrULU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cnb9SnSVGP96"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFCQsLm8GP97"
   },
   "source": [
    "Note that we no longer call ``log_softmax`` in the ``model`` function. Let's confirm that our loss and accuracy are the same as before:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g22boR9RGP97"
   },
   "outputs": [],
   "source": [
    "### ENTER YOUR CODE HERE (2 line expected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ4oD7VZGP97"
   },
   "source": [
    "## Exercise 1.5 Refactor using nn.Module\n",
    "Next up, we'll use ``nn.Module`` (*uppercase M*) and ``nn.Parameter``, for a clearer and more concise training loop.\n",
    "1. We subclass ``nn.Module`` (which itself is a class and able to keep track of its attributes).  In this case, we want to create a class that holds our weights, bias, and method for the forward step.  ``nn.Module`` has a number of attributes and methods (such as ``.parameters()`` and ``.zero_grad()``) which we will be using for this.\n",
    "1. Since we're now using an object instead of just using a function, we\n",
    "first have to instantiate our model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGBBDsleGP97"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(n_features, n_classes) / np.sqrt(n_features))\n",
    "        self.bias = nn.Parameter(torch.randn(n_classes) / np.sqrt(n_features))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias\n",
    "\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Pnf7Nu4GP98"
   },
   "source": [
    "Now we can calculate the loss in the same way as before. Note that ``nn.Module`` objects are used as if they are functions (i.e they are *callable*), but behind the scenes Pytorch will call our ``forward`` method automatically, i.e.\n",
    "```\n",
    "  def __call__(self, *args, **kwargs):\n",
    "    return self.forward(args, kwargs)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BjtCUYrvGP98"
   },
   "outputs": [],
   "source": [
    "### ENTER YOUR CODE HERE (1 line expected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFocHmEIGP98"
   },
   "source": [
    "#### model.parameters() and model.zero_grad()\n",
    "\n",
    "Previously for our training loop we had to update the values for each parameter\n",
    "by name, and manually zero out the grads for each parameter separately, like this:\n",
    "```\n",
    "  with torch.no_grad():\n",
    "      weights -= weights.grad * lr\n",
    "      bias -= bias.grad * lr\n",
    "      weights.grad.zero_()\n",
    "      bias.grad.zero_()\n",
    "```\n",
    "\n",
    "Now we can take advantage of ``model.parameters()`` and ``model.zero_grad()`` to make those steps more concise and less prone to the error of forgetting some of our parameters, particularly if we had a more complicated model:\n",
    "```\n",
    "  with torch.no_grad():\n",
    "      for p in model.parameters():\n",
    "          p -= p.grad * lr\n",
    "      model.zero_grad()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll wrap our little training loop in a ``fit`` function so we can run it again later. And we double-check that our loss has gone down:"
   ],
   "metadata": {
    "id": "_B-qAGBQK6kw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvkInV04GP98"
   },
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((train_samples - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            ### ENTER YOUR CODE HERE (4 lines expected)\n",
    "\n",
    "\n",
    "        print(f\"Epoch: {epoch}, {loss_func(model(x_train), y_train):.2f}\")\n",
    "\n",
    "fit()\n",
    "print(f\"Accuracy: {accuracy(model(x_train), y_train):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzMBuDeFGP99"
   },
   "source": [
    "## Exercise 1.6 Refactor using nn.Linear\n",
    "\n",
    "We continue to refactor our code.  Instead of manually defining and initializing ``self.weights`` and ``self.bias``, and calculating ``xb  @ self.weights + self.bias``, we will instead use the Pytorch class [nn.Linear](https://pytorch.org/docs/stable/nn.html#linear-layers)(n_features, n_classes) for a linear layer, which does all that for us.\n",
    "\n",
    "Pytorch has many types of **predefined layers** that can greatly simplify our code, and often makes it faster too!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSnmEI1JGP99"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(n_feats, n_cls)\n",
    "        ### ENTER YOUR CODE HERE (1 line expected)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        ### ENTER YOUR CODE HERE (1 lines expected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFS60dpFGP99"
   },
   "source": [
    "We instantiate our model and calculate the loss in the same way as before:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFW4bMSUGP99"
   },
   "outputs": [],
   "source": [
    "### ENTER YOUR CODE HERE (2 lines expected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZJfkdWmGP9-"
   },
   "source": [
    "We are still able to use our same ``fit`` method as before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zau12rXJGP9-"
   },
   "outputs": [],
   "source": [
    "fit()\n",
    "print(f\"Loss: {loss_func(model(x_train), y_train)}, Accuracy: {accuracy(model(x_train), y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHBvm9BMGP9-"
   },
   "source": [
    "## Exercise 1.7 Refactor using optim\n",
    "\n",
    "Pytorch also has a package with various optimization algorithms, ``torch.optim``. To implement the previous stochastic gradient descent (SGD) optimization step, we can use ``optim.SGD(params, lr)`` passing to it all the model parameters and the learning rate.\n",
    "\n",
    "We can use the ``step`` method from our optimizer to take a forward step, instead of manually updating each parameter.\n",
    "\n",
    "This will let us replace our previous manually coded optimization step:\n",
    "```\n",
    "  with torch.no_grad():\n",
    "      for p in model.parameters():\n",
    "          p -= p.grad * lr\n",
    "      model.zero_grad()\n",
    "\n",
    "```\n",
    "\n",
    "and instead use just:\n",
    "```\n",
    "# before the training loop\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# in the training loop\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "```\n",
    "\n",
    "``optim.zero_grad()`` resets the gradient to 0 and we need to call it **before** computing the gradient for the next minibatch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4wtsLH1GP9-"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "model = LogisticRegression(n_features, n_classes)\n",
    "\n",
    "### ENTER YOUR CODE HERE (1 line expected)\n",
    "\n",
    "print(f\"Accuracy: {accuracy(model(x_train), y_train)}\")\n",
    "\n",
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((train_samples - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            ### ENTER YOUR CODE HERE (2 lines expected)\n",
    "\n",
    "\n",
    "        print(f\"Epoch: {epoch}, {loss_func(model(x_train), y_train)}\")\n",
    "\n",
    "fit()\n",
    "print(f\"Accuracy: {accuracy(model(x_train), y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAa3H3EkGP9_"
   },
   "source": [
    "## Exercise 1.8 Refactor using Dataset\n",
    "\n",
    "PyTorch has an abstract Dataset class in ``torch.utils.data.Dataset``.  A Dataset can be anything that has:\n",
    "*   a ``__len__`` function (called by Python's standard ``len`` function)\n",
    "*   a ``__getitem__`` function as a way of indexing into it.\n",
    "\n",
    "There are many pre-existing datasets in the [torchvision](https://pytorch.org/vision/stable/datasets.html) library. Otherwise you can also create your own.\n",
    "[This tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) walks through a nice example of creating a custom ``FacialLandmarkDataset`` class as a subclass of ``Dataset``.\n",
    "\n",
    "PyTorch's [TensorDataset](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset) is a Dataset wrapping tensors. By defining a length and way of indexing, this also gives us a way to iterate, index, and slice along the first dimension of a tensor.\n",
    "\n",
    "Both ``x_train`` and ``y_train`` can be combined in a single ``TensorDataset``, which will be easier to iterate over and slice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7jC6HSPGP9_"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "train_ds = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvKdmDrzGP9_"
   },
   "source": [
    "Previously, we had to iterate through minibatches of x and y values separately:\n",
    "```\n",
    "    xb = x_train[start_i:end_i]\n",
    "    yb = y_train[start_i:end_i]\n",
    "```\n",
    "\n",
    "Now, we can do these two steps together:\n",
    "```\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs] # we access all tensors in the dataset with one slicing\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hh3uQWbgGP9_"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(n_features, n_classes)\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "print(f\"Accuracy: {accuracy(model(x_train), y_train)}\")\n",
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for i in range((train_samples - 1) // bs + 1):\n",
    "\n",
    "        ### ENTER YOUR CODE HERE (1 line expected)\n",
    "\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        print(f\"Epoch: {epoch}, {loss_func(model(x_train), y_train)}\")\n",
    "\n",
    "fit()\n",
    "print(f\"Accuracy: {accuracy(model(x_train), y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spVPEQSbGP9_"
   },
   "source": [
    "## Exercise 1.9 Refactor using DataLoader\n",
    "\n",
    "Pytorch's ``DataLoader`` is responsible for managing batches. You can create a ``DataLoader`` from any ``Dataset``. ``DataLoader`` makes it easier to iterate over batches. Rather than having to use ``train_ds[i*bs : i*bs+bs]``, the DataLoader gives us each minibatch automatically.\n",
    "\n",
    "Also Shuffling the training data is [important](https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks) to prevent correlation between batches and overfitting. DataLoader provides the parameter ``shuffle=True`` to do it.\n",
    "\n",
    "For data intensive training (e.g. big images, or videos), it also allows to employ multiprocessing to load the data in parallel to the training loop with the parameter ``num_workers``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUKoDBjtGP-A"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzWYd4sjGP-A"
   },
   "source": [
    "Previously, our loop iterated over batches (xb, yb) like this:\n",
    "\n",
    "      for i in range((n-1)//bs + 1):\n",
    "          xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "          pred = model(xb)\n",
    "\n",
    "Now, our loop is much cleaner, as (xb, yb) are loaded automatically from the data loader:\n",
    "\n",
    "      for xb,yb in train_dl:\n",
    "          pred = model(xb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBvpcKS4GP-A"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "print(f\"Accuracy: {accuracy(model(x_train), y_train)}\")\n",
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        ### ENTER YOUR CODE HERE (2 lines expected)\n",
    "\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        print(f\"Epoch: {epoch}, {loss_func(model(x_train), y_train)}\")\n",
    "\n",
    "fit()\n",
    "print(f\"Accuracy: {accuracy(model(x_train), y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_RoAmr7GP-A"
   },
   "source": [
    "Thanks to Pytorch's ``nn.Module``, ``nn.Parameter``, ``Dataset``, and ``DataLoader``,\n",
    "our training loop is now dramatically smaller and easier to understand. Let's\n",
    "now try to add the basic features necessary to create effective models in practice.\n",
    "\n",
    "## Exercise 1.10 Add validation\n",
    "\n",
    "So far, we were just trying to get a reasonable training loop set up for use on our training data.  In reality, you **always** should also have a [validation set](https://www.fast.ai/2017/11/13/validation-sets/), in order to identify if you are overfitting. We will extract it from the training set and use it to evaluate the model at the end of each epoch.\n",
    "\n",
    "We'll use a batch size for the validation set that is twice as large as that for the training set. This is because the validation set does not need backpropagation and thus takes less memory (it doesn't need to store the gradients). We take advantage of this to use a larger batch size and compute the loss more quickly by inserting the validation loop in a ``with torch.no_grad():``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeNLS88OGP-A"
   },
   "outputs": [],
   "source": [
    "### ENTER YOUR CODE HERE (5 lines expected),\n",
    "\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dl)}, valid samples: {len(valid_dl)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KoFuQxUGP-A"
   },
   "source": [
    "We will calculate and print the validation loss at the end of each epoch.\n",
    "\n",
    "Note that we always call ``model.train()`` before training, and ``model.eval()`` before inference, because these are used by some layers (not employed here) such as ``nn.BatchNorm2d`` and ``nn.Dropout`` to ensure appropriate behaviour for these different phases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGVGM59HGP-A"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        ### ENTER YOUR CODE HERE (1 line expected)\n",
    "\n",
    "        for xb, yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "\n",
    "        ### ENTER YOUR CODE HERE (6-8 lines expected)\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch:.2f}: Val Loss {np.mean(valid_losses):.2f}, \"\n",
    "              f\"Val Acc: {np.mean(valid_accs):.2f}\")\n",
    "\n",
    "fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1.11 Saving best model: early stopping\n",
    "It may have happend that the model has reached the highest validation accuracy (and/or lower validation loss) not at the last epoch. It means that the model overfitted the training data.\n",
    "\n",
    "A possible way to avoid this phenomenon is to save the model achieving the best validation accuracy (other possible solutions includes reducing the learning rate and decreasing the number of training epochs).\n",
    "\n",
    "There exists two possible way to do it in pytorch. ``torch.save()`` and ``torch.load()`` can be used with any kind of objects. Torch will serialized this object through pickle. However, if the code generating the object is modified the code might brake in several ways. To avoid this issue, torch provides another couple of functions, ``model.state_dict()`` and ``model.load_state_dict()``, which only save and load the weights of the network.\n",
    "\n",
    "    torch.save(model.state_dict(), \"./best_model.pt\")\n",
    "    model.load_state_dict(torch.load(\"./best_model.pt\"))\n",
    "\n",
    "For further information on this topic, please read this [tutorial](https://pytorch.org/tutorials/beginner/saving_loading_models.html).\n"
   ],
   "metadata": {
    "id": "ccS_IWFCEPTf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = LogisticRegression()\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "def fit():\n",
    "    best_acc = 0.\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_losses, valid_accs = [], []\n",
    "            for xb, yb in valid_dl:\n",
    "                valid_losses.append(loss_func(model(xb), yb))\n",
    "                valid_accs.append(accuracy(model(xb), yb))\n",
    "\n",
    "            if best_acc < np.mean(valid_accs):\n",
    "\n",
    "                ### ENTER YOUR CODE HERE - SAVE MODEL (1 line expected)\n",
    "\n",
    "                best_acc = np.mean(valid_accs)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Loss {np.mean(valid_losses):.2f},  \\\n",
    "                Acc: {np.mean(valid_accs):.2f}\")\n",
    "\n",
    "\n",
    "    ### ENTER YOUR CODE HERE (2 lines expected)\n",
    "\n",
    "\n",
    "    print(f\"Best Valid Acc: {np.mean([accuracy(model(xb), yb) for xb, yb in valid_dl]):.2f}\")\n",
    "\n",
    "fit()\n"
   ],
   "metadata": {
    "id": "q7uoHLFmUzRk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Was it useful to early stop the training? Is the best accuracy the same as the one at the last epoch?\n",
    "\n",
    "Finally, we test the model on the test set to see if it generalizes well also there.\n",
    "**Note**: There might be a discrepancy between the validation and test accuracy, but it should **not** be too large."
   ],
   "metadata": {
    "collapsed": false,
    "id": "ESKhCTskFQvk"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "\n",
    "### ENTER YOUR CODE HERE (2 lines expected)\n"
   ],
   "metadata": {
    "id": "NVo8bdFiFQvk"
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEADjDjZGP-E"
   },
   "source": [
    "## Exercise 1.12 Using the GPU\n",
    "\n",
    "If you're lucky enough to have access to a CUDA-capable GPU you can use it to speed up your code. To do so in COLAB you can change *runtime type* in the *Runtime* dropdown menu to enable GPU computation. After changing the runtime type you will need to rerun the whole notebook beacause all variables will be lost (*Runtime* -> *Run all*).\n",
    "\n",
    "Let's check that your GPU is working with Pytorch. It should print ``True`` if everything is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0jas1PPGP-E"
   },
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkSz6FMDGP-E"
   },
   "source": [
    "Let's then create a device object for it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFnllnDDGP-E"
   },
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfvKuUCuGP-F"
   },
   "source": [
    "Finally, we can move our model and our data to the GPU.\n",
    "Generally dataset do not fit in the GPU, so we need to move only the batches inside the training/validation loops:\n",
    "\n",
    "    xb = xb.to(dev)\n",
    "    yb = yb.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGAygmyqGP-F"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression().to(dev)\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "def fit():\n",
    "    best_acc = 0.\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            ### ENTER YOUR CODE HERE (1 line expected)\n",
    "\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_losses, valid_accs = [], []\n",
    "            for xb, yb in valid_dl:\n",
    "                ### ENTER YOUR CODE HERE (1 line expected)\n",
    "\n",
    "                valid_losses.append(loss_func(model(xb), yb).cpu())\n",
    "                valid_accs.append(accuracy(model(xb), yb).cpu())\n",
    "\n",
    "            if best_acc < np.mean(valid_accs):\n",
    "                torch.save(model.state_dict(), \"./best_model.pt\")\n",
    "                best_acc = np.mean(valid_accs)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Loss {np.mean(valid_losses):.2f}, \"\n",
    "              f\"Acc: {np.mean(valid_accs):.2f}\")\n",
    "    model.load_state_dict(torch.load(\"./best_model.pt\"))\n",
    "    model.eval()\n",
    "    print(f\"Best Valid Acc: {np.mean([accuracy(model(xb.to(dev)), yb.to(dev)).cpu() for xb, yb in valid_dl]):.2f}\")\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PigzqlxyGP-F"
   },
   "source": [
    "You should find it runs faster now\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 2 What has the network learnt?\n",
    "\n",
    "So far we have seen that the network has learnt, but how can we visualize it?\n",
    "There exists several mechanism to do so.\n"
   ],
   "metadata": {
    "id": "Oz2QEZ0h0ZBJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Exercise 2.1 Weight visualization\n",
    "On a logistic regression we can directly visualize both biases and weights! We will reuse the same ``im.show()`` function that we used before to visualize now the learnt weights.\n",
    "\n"
   ],
   "metadata": {
    "id": "a6QENkC-Fye1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "learnt_bias = model.lin.bias.cpu().detach()\n",
    "learnt_weights = model.lin.weight.cpu().detach()\n",
    "print(f\"Bias: {learnt_bias}\")\n",
    "for i in range(10):\n",
    "  ### ENTER YOUR CODE HERE (1 line expected)\n",
    "\n",
    "  plt.title(f\"Prototype of the {i} class\")\n",
    "  plt.show()"
   ],
   "metadata": {
    "id": "pQfAWYIX0Ydz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The images are surely blurry but they resemble somehow the standard patterns of the 0-9 digits"
   ],
   "metadata": {
    "id": "Ov1X1VV9OV_r"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2.1 T-SNE weight projection\n",
    "\n",
    "Otherwise with more complex model you can still see the distribution of the classes as learnt by the model through some visualization techniques. Personally I like very much [t-sne](https://lvdmaaten.github.io/tsne/): it allows to project into low dimensional representation (e.g. 2D) data which lies in very high dimensional spaces.\n",
    "\n",
    "Let's try to projects the learnt weights into a 2D space.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "Is7Z94v83efi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "projected_weights = TSNE(n_components=2,init=\"pca\", learning_rate='auto', perplexity=5).fit_transform(learnt_weights)\n",
    "\n",
    "sns.scatterplot(x=projected_weights[:,0], y=projected_weights[:,1], hue=(str(i) for i in range(10)), legend=False)\n",
    "[plt.text(projected_weights[i,0], projected_weights[i,1], s=str(i)) for i in range(10)]\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "UVPJ69J44gyL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "But you can do it with the samples as well!\n",
    "Let's do it with the samples in the test set\n"
   ],
   "metadata": {
    "id": "wlmezIow6cZy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "projected_samples = TSNE(n_components=2, init=\"pca\", learning_rate='auto').fit_transform(x_test)\n",
    "\n",
    "sns.scatterplot(x=projected_samples[:,0], y=projected_samples[:,1], hue=[str(y_i.item()) for y_i in y_test])\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "q2nJUIJl6b-V"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NFq0fhbGP-F"
   },
   "source": [
    "# Closing thoughts\n",
    "\n",
    "We now have a general data pipeline and training loop which you can use for\n",
    "training many types of models using Pytorch.\n",
    "\n",
    "We promised at the start of this tutorial we'd explain through example each of\n",
    "``torch.nn``, ``torch.optim``, ``Dataset``, and ``DataLoader``. So let's summarize what we've seen:\n",
    "\n",
    " - **torch.nn**\n",
    "\n",
    "   + ``Module``: creates a callable which behaves like a function, but can also\n",
    "     contain state(such as neural net layer weights). It knows what ``Parameter`` (s) it\n",
    "     contains and can zero all their gradients, loop through them for weight updates, etc.\n",
    "   + ``Parameter``: a wrapper for a tensor that tells a ``Module`` that it has weights\n",
    "     that need updating during backprop. Only tensors with the `requires_grad` attribute set are updated\n",
    "   + ``functional``: a module(usually imported into the ``F`` namespace by convention)\n",
    "     which contains activation functions, loss functions, etc, as well as non-stateful\n",
    "     versions of layers such as convolutional and linear layers.\n",
    " - ``torch.optim``: Contains optimizers such as ``SGD``, which update the weights\n",
    "   of ``Parameter`` during the backward step\n",
    " - ``Dataset``: An abstract interface of objects with a ``__len__`` and a ``__getitem__``,\n",
    "   including classes provided with Pytorch such as ``TensorDataset``\n",
    " - ``DataLoader``: Takes any ``Dataset`` and creates an iterator which returns batches of data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "gAa3H3EkGP9_",
    "spVPEQSbGP9_",
    "J_RoAmr7GP-A",
    "FEADjDjZGP-E",
    "6NFq0fhbGP-F"
   ],
   "toc_visible": true
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
