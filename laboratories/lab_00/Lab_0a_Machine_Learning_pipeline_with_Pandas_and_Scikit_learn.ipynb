{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqNdhAUAJZsb"
   },
   "source": [
    "# **Lab 0 - Explainable and Trustworthy AI**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfgzpidqJrNF"
   },
   "source": [
    "**Teaching Assistant**: *Salvatore Greco*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXQgTsYkJtDc"
   },
   "source": [
    "## **Lab 0:** Machine Learning pipeline with Pandas and Scikit-learn\n",
    "\n",
    "In this lab, you will learn about **pre-processing** and **model training** in Machine Learning (ML) with [Pandas](https://pandas.pydata.org/) and [Scikit-Learn](https://scikit-learn.org/stable/) libraries.\n",
    "\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/) is a Python library useful for handling and analyzing data structures, particularly bidimensional tables and time series (i.e., data associated with time). It provides useful data structures (e.g., Series and DataFrames) to manage data effectively. The library provides tools for managing the data selection, transforming data with grouping and pivoting operations, managing missing data in the dataset, and performing statistics and charts on data. Pandas is based on [Numpy](https://numpy.org/) arrays.\n",
    "\n",
    "[Scikit-Learn](https://scikit-learn.org/stable/) is a Python library that implements many machine learning algorithms, and it is built on [Numpy](https://numpy.org/), [SciPy](https://scipy.org/) and [Matplotlib](https://matplotlib.org/). In Scikit-learn both *unsupervised* (e.g., K-Means, DBScan clustering algorithms), and *supervised* algorithms for *regression* and *classification* tasks are available. Scikit-Learn also provides  useful functions for data pre-processing, feature extraction, feature selection, and dimensionality reduction.\n",
    "\n",
    "A typical **machine learning pipeline** involves the following steps:\n",
    "  1. **Data Collection**: Gather your data. - *(uncovered)*\n",
    "  2. **Data Exploration**: Perform exploratory data analysis to understand patterns, distributions, and correlations in the data. - *(uncovered)*\n",
    "  3. **Data Splitting**: Split the dataset into training, validation (optional), and test sets.\n",
    "  4. **Data Cleaning**: Handle missing values, remove duplicates, and correct errors.\n",
    "  5. **Feature Selection**: Select relevant features and remove redundant ones.\n",
    "  6. **Data Transformation**: Normalization, standardization, and encoding.\n",
    "  7. **Feature Engineering**: Create new features or modify existing ones (e.g., discretization).\n",
    "  8. **Data Augmentation**: Augment the training set to increase its size and variability (if possible). Apply techniques like oversampling, undersampling, or [SMOTE](https://medium.com/@corymaklin/synthetic-minority-over-sampling-technique-smote-7d419696b88c) to handle imbalanced data. - *(uncovered)*\n",
    "  9. **Model Selection and Training**: Choose and train the model using the pre-processed training set.\n",
    "  10. **Hyperparameters Tuning**: Explore various hyperparameter configurations to improve upon the baseline model's performance.  Evaluate each set of hyperparameters using a validation set or cross-validation to assess the model's performance. - *(uncovered)*\n",
    "  11. **Model Evaluation**: Evaluate the model's performance on the preprocessed test set using appropriate metrics.\n",
    "\n",
    "You can also create pre-processing pipelines that automate all the pre-processing steps.\n",
    "\n",
    "The previous steps are just a general list. However, they depend on the model you want to train. For example, tree-based algorithms such as decision trees and random forests can handle categorical data naturally. This, they do not require the encoding of categorical features and normalization/standardization.\n",
    "\n",
    " Note that, it is reccomended to split the dataset early in the process and using *only* the training set for deriving any data-specific insights or transformations are fundamental practices to prevent data leakage and ensure the model's generalizability to new data. This approach maintains the test set as an unbiased assessment of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zj7UfwlI2odb"
   },
   "source": [
    "---\n",
    "\n",
    "## **Exercise 1: Titanic Survival Prediction**\n",
    "\n",
    "In this exercise, you will train a binary classification model that predicts which **passengers survived** the **Titanic shipwreck** <a href=\"https://www.kaggle.com/c/titanic\" >link</a>.\n",
    "\n",
    "The sinking of the Titanic is one of the most famous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
    "\n",
    "While some element of luck was involved in surviving, it seems some groups of people were more likely to survive than others.\n",
    "\n",
    "In this exercise, you are asked to build a predictive model that answers the question: *“What sorts of people were more likely to survive?”* using passenger data (i.e., name, age, gender, socio-economic class, etc).\n",
    "\n",
    "You can find two detailed **tutorials** in the following links: [tutorial1](https://datasciencewithchris.com/kaggle-titanic-data-cleaning-and-preprocessing/) and [tutorial2](https://medium.com/@melodyyip_/titanic-survival-prediction-using-machine-learning-89a779656113).\n",
    "\n",
    "Run the next cell to import the required libraries for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CwB2Ce64HK_"
   },
   "outputs": [],
   "source": [
    "# Import the required libraries for this exercise\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import tree\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMkUzijj3mD3"
   },
   "source": [
    "### 1.1 Load dataset\n",
    "\n",
    "Firstly, you will load the **Titanic** dataset used in this lab into a DataFrame `df`.\n",
    "\n",
    "**Scikit-Learn** comes with a built-in dataset for the **Titanic survival prediction** task. The next cell loads the titanic dataset from Scikit-Learn and stores it in a Pandas [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKSp_vwe3llL",
    "outputId": "b92c52bc-8020-43d7-9972-eb243645d998"
   },
   "outputs": [],
   "source": [
    "# Load input features and target variable\n",
    "df, y = fetch_openml('titanic', version=1, as_frame=True, parser='auto', return_X_y=True)\n",
    "\n",
    "# The \"survived\" column contains the target variable\n",
    "df[\"survived\"] = y\n",
    "\n",
    "# Print the number of samples in the dataset\n",
    "print(f\"Number of samples in the dataset: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aFWy-Rx4xtS"
   },
   "source": [
    "Pandas [DataFrames](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) have useful methods and attributes  to manipulate and analyze data efficiently.\n",
    "\n",
    "Some methods and attributes are useful for getting a quick overview of your data. Some examples include:\n",
    "- `df.head()`: This method returns the first *n* rows of the DataFrame, where *n* is a parameter that you can specify. If you do not specify n, it defaults to 5. This is particularly useful for quickly inspecting the beginning of your dataset to understand its structure and the type of data it contains.\n",
    "- `df.info()`: This method provides a concise summary of the DataFrame, including the number of non-null entries in each column, the data type of each column, the memory usage, the number of columns, and the range index. It can be useful for getting a quick overview of the DataFrame's structure and to identify columns with missing values.\n",
    "- `df.columns()`: This attribute can be used to view or modify the column names. For example, you can use df.columns.tolist() to get a list of all column names.\n",
    "- `df.describe()`: This method generates descriptive statistics that summarize the central tendency, dispersion, and shape of the dataset’s distribution, excluding *NaN* values. It works on numeric and object data types, providing information such as mean, standard deviation, minimum, maximum, and quartile values for numeric data, and count, unique, top, and frequency for object data (e.g., strings or timestamps).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "eAIowKzY2nk7",
    "outputId": "5cd63c47-c039-4392-b71b-1f95ea711e97"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DE-Rk2o64gwE",
    "outputId": "9b6f3ef8-5351-42bb-9bcb-3a610731e067"
   },
   "outputs": [],
   "source": [
    "print(f\"Dataset columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kBHWPBQ_42Y0",
    "outputId": "f1ecda1d-c2c2-4e5f-fda3-c5d85240f7f0"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "qLhQvKqc4-i7",
    "outputId": "29c26b65-f793-4d76-ef2c-016deca619ec"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qL9Cc3Jb58Vn"
   },
   "source": [
    "The `\"survived\"` column contains the target variable (i.e., the variable that you want to predict).\n",
    "\n",
    "Some datasets contain a **balanced** number of samples for each label. Thus, each category of data is equally represented. However, many real-world datasets are **imbalanced**, meaning they have a disproportionate number of samples in one or more classes than others.\n",
    "\n",
    "Highly **imbalanced** datasets can cause the model to become biased towards the more frequently represented class(es), thereby reducing the model's ability to generalize well across all categories. In such cases, the model trained may perform well on the majority class(es) but poorly on the minority class(es), because it has not had enough data to learn from for the underrepresented categories. Imbalance can significantly affect the performance and fairness of predictive models, leading to misleadingly high accuracy scores that do not accurately reflect the model's ability to predict less frequent classes.\n",
    "\n",
    "Run the next cell to count the number of samples for each class label. This is useful to verify if the dataset is **balanced** or **imbalanced**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5tb6K1O58rA",
    "outputId": "b1632397-b628-4336-8f01-ad7e4db3e630"
   },
   "outputs": [],
   "source": [
    "df[\"survived\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuVLnPYi6TeQ"
   },
   "source": [
    "In this case, the dataset is slightly **imbalanced**. The *non-survived* class (0) is more frequent than the *survived* class (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2smeP5gJq86S"
   },
   "source": [
    "The next cell counts the number of duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ASqL9o1_opOd",
    "outputId": "63006dcb-c3fe-456c-ec68-8b486b5a1c00"
   },
   "outputs": [],
   "source": [
    "# check for duplicate rows\n",
    "duplicate_rows = df.duplicated(keep=False).sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pi1LvAkrCOZ"
   },
   "source": [
    "There are no duplicate rows in this dataset. However, in Pandas, you can remove duplicate rows using `df.drop_duplicates()`. You can also remove duplicates based on a specified column `df.drop_duplicates(subset='column_name')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQYZYevp5EvG"
   },
   "source": [
    "### 1.2 Train and Test splitting with Stratification\n",
    "\n",
    "The first step involves splitting your dataset into distinct subsets to ensure that your model can generalize well to unseen data. This step is crucial for evaluating the performance of your model in an unbiased manner.\n",
    "\n",
    "Datasets are usually split into the following subset:\n",
    "- **Training Set**: Subset of your data used to train your model. It is the largest portion from which your model learns the underlying patterns to perform accurate predictions.\n",
    "\n",
    "- **Validation Set**: (Optional but highly recommended) Subset used to fine-tune the model's hyperparameters and evaluate which models, configurations, or hyperparameters is the best performance. It acts as a proxy for the test set during the development phase.\n",
    "\n",
    "- **Test Set**: Subset used to evaluate the final model's performance after it has been trained and validated. It provides an assessment of how well your model has learned to generalize from the training data to new, unseen data.\n",
    "\n",
    "In this lab, we will only use training and test set for semplicity, and due to the low number of samples in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOktJVoEKWw-"
   },
   "source": [
    "**Exercise:** Split the dataset into **train** and **test** datasets. In this case, the dataset is **imbalanced**. Therefore, it is recommended to split using stratification (i.e., the class label distribution will be preserved during the splitting).\n",
    "\n",
    "**Split** with 80% of samples for training and 20% of samples for validation. **Shuffle** the dataset before splitting, and perform the **stratification** by label. Replace `None` with your code.\n",
    "\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li> To split a dataset you can use the `train_test_split` function of Scikit-learn (<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\" >link</a>).\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRxdztEq6uzQ"
   },
   "outputs": [],
   "source": [
    "#### START CODE HERE (~ 1 line) ####\n",
    "\n",
    "df_train, df_test = None\n",
    "\n",
    "#### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x84gK3fI7Gvc",
    "outputId": "191646de-b226-4e68-e21d-0b404c995ac9"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of samples in the training set {len(df_train)}\")\n",
    "print(df_train[\"survived\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XYw0lXuP7idw",
    "outputId": "3eb59b95-57d5-4d12-f42f-c79825f8651d"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of samples in the test set {len(df_test)}\")\n",
    "print(df_test[\"survived\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AtbuX6z7wiF"
   },
   "source": [
    "### 1.3 Handling missing values\n",
    "\n",
    "Machine learning algorithms require that all the input values are in a **numerical** formal. However, real-world datasets are often \"dirty\". For instance, they can contain missing values for some columns and records. Before training your ML models, you should handle missing values.\n",
    "\n",
    "You should first check if **null** values are present in your dataset. Pandas Dataframes have many useful methods to check for null values in your dataset.\n",
    "- `df.isnull()` or `df.isna()`: They return a DataFrame with the same shape as the input DataFrame, but containing boolean values (True or False) indicating the presence of null values.\n",
    "- `df.notnull()` or `df.notna()`: The opposite of isnull() and isna().\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjhO1z9QKiIU"
   },
   "source": [
    "**Exercise**: Count the number of **null values** in training and test, and store them in the variables `nan_count_train` and `nan_count_test`.\n",
    "Replace `None` with your code.\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "\n",
    "<ul>\n",
    "    <li> Remember that boolean values can also be interpreted as 0 (False) and 1 (True). Thus, you can exploit the `df.sum()` method to count the number of ones.\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfZSM9H68Ln0"
   },
   "outputs": [],
   "source": [
    "#### START CODE HERE (~ 2 line) ####\n",
    "\n",
    "nan_count_train = None\n",
    "nan_count_test = None\n",
    "\n",
    "#### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GH7B7adZVP51",
    "outputId": "c2065fbe-5730-4bf1-9b04-b205cc39ff83"
   },
   "outputs": [],
   "source": [
    "print(\"Train\")\n",
    "print(nan_count_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e17C-cfEVe0D",
    "outputId": "003d7a4c-fc54-41c6-b676-5648301a3f6f"
   },
   "outputs": [],
   "source": [
    "print(\"Test\")\n",
    "print(nan_count_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vj30nIBUVkNp"
   },
   "source": [
    "In several columns of the dataset, missing values are present, specified with `NaN` (i.e., not a number).\n",
    "\n",
    "There are several strategies for handling missing data, some examples include:\n",
    "1. **Deletion**: Discard entire rows/columns containing missing\n",
    "values.\n",
    "2.  **Imputation**: Replace missing values with some imputed values (e.g., mean, median, constant, etc.).\n",
    "\n",
    "3.  **Inference**: Use other data points to train a model that can predict the missing values.\n",
    "\n",
    "> #### 1. Discarding missing values\n",
    "  * You can **remove** rows or columns containing missing values using the `df.dropna(axis=)` method of Pandas DataFrames. If you specify `axis=0`, it will remove *rows* containing missing values. In contrast, if you specify `axis=1`, it will remove the *columns* containing missing values.\n",
    "  * You can also remove rows containing missing values in a specific column specifying the `subset` parameter (e.g., `df.dropna(subset = [\"column_name\"])`). In this case,  all rows containing a missing value in the `column_name` column are removed.\n",
    "  * Note that, `df.dropna()` returns a new DataFrame. Therefore, you should re-assign to `df` the new DataFrame (e.g., `df = df.dropna()`) or set the `inplace` parameter to `True` (e.g., `df.dropna(inplace=True)`).\n",
    "\n",
    "> #### 2. Imputing missing values\n",
    "  * You can impute values on missing data with Pandas with the `df.fillna()` method and specify the new value that will replace the `NaN` values. The `df.fillna()` method returns a new DataFrame by replacing the null values with the specified value. For instance, you can replace `NaN` values with the column mean with `df.fillna(df.mean())`.\n",
    "  * You can also use Scikit-Learn to impute values on missing data with `sklearn.impute.SimpleImputer`. The [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) can replace missing values using a descriptive statistic (e.g., mean, median, or most frequent) along each column, or using a constant value.\n",
    "    - `\"mean\"`: replace missing values using the *mean* along each column (only for numeric data).\n",
    "    - `\"median\"`: replace missing values using the *median* along each column (only for numeric data).\n",
    "    - `\"most_frequent\"`: replace missing using the *most frequent value* along each column (for both strings and numeric data).\n",
    "  * Below is reported an example of usage:\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Instantiate a SimpleImputer object specifying the descriptive statistic\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean’)\n",
    "\n",
    "# Compute the mean fitting on training data (important! do not fit on test data)\n",
    "imp_mean.fit(X_train.values)  \n",
    "\n",
    "# Replace missing values in the training set\n",
    "X_train = imp_mean.transform(X_train.values)\n",
    "# replace missing values in the test set\n",
    "X_test = imp_mean.transform(X_ test.values)\n",
    "\n",
    "```\n",
    "\n",
    "> #### 3. Predicting missing values\n",
    "Using models to predict the missing values is uncovered in this lab. However, the idea is to simply train a machine learning model (e.g., linear regression) to predict missing values. If you are interested, you can read more about it [here](https://medium.com/machine-learning-mastery/5-ways-to-handle-missing-values-in-python-4fe6a625e251)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eA2Aywj2iPjl"
   },
   "source": [
    "**Exercise**: Fill **null values** in the column `age` with the **mean** of the column `age` in the training and test set.\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "\n",
    "<ul>\n",
    "    <li> Remember that all the statistics must be computed on the training set only. Therefore, the null values in the test set must be replaced by the mean of the training.\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FDaNsn6iObL",
    "outputId": "3cb961de-af8a-4659-d5d7-fc2c4ebe76ec"
   },
   "outputs": [],
   "source": [
    "print(f'Number of null values in Train before pre-processing: {df_train.age.isnull().sum()}/{len(df_train)}')\n",
    "print(f'Number of null values in Test before pre-processing: {df_test.age.isnull().sum()}/{len(df_test)}')\n",
    "\n",
    "#### START CODE HERE (~ 2 line) ####\n",
    "\n",
    "\n",
    "#### END CODE HERE ####\n",
    "\n",
    "print(f'Number of null values in Train after pre-processing: {df_train.age.isnull().sum()}/{len(df_train)}')\n",
    "print(f'Number of null values in Test after pre-processing: {df_test.age.isnull().sum()}/{len(df_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8ss19yvNLGS"
   },
   "source": [
    "**Exercise**: Fill **null values** in the column `fare` with the **median** of the column `fare` in the training and test set.\n",
    "\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "\n",
    "<ul>\n",
    "    <li> Remember that all the statistics must be computed on the training set only. Therefore, the null values in the test set must be replaced by the median of the training.\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tsNfDvT9NJbT",
    "outputId": "3be6f7d3-1b13-4ab8-e6d3-4f1b3d4977cd"
   },
   "outputs": [],
   "source": [
    "print(f'Number of null values in Train before pre-processing: {df_train.fare.isnull().sum()}/{len(df_train)}')\n",
    "print(f'Number of null values in Test before pre-processing: {df_test.fare.isnull().sum()}/{len(df_test)}')\n",
    "\n",
    "#### START CODE HERE (~ 2 line) ####\n",
    "\n",
    "\n",
    "#### END CODE HERE ####\n",
    "\n",
    "print(f'Number of null values in Train after pre-processing: {df_train.fare.isnull().sum()}/{len(df_train)}')\n",
    "print(f'Number of null values in Test after pre-processing: {df_test.fare.isnull().sum()}/{len(df_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PA0be74BNeT2"
   },
   "source": [
    "**Exercise**: Fill **null values** in the column `embarked` with the **most frequent value** of the column `embarked` in the training and test set.\n",
    "\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "\n",
    "<ul>\n",
    "    <li> Remember that all the statistics must be computed on the training set only. Therefore, the null values in the test set must be replaced by the most frequent value of the training.\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "HfTMZV8bNpRq",
    "outputId": "90a8c63f-bfc2-46ed-e20b-0369cb4f0469"
   },
   "outputs": [],
   "source": [
    "print(f'Number of null values in Train before pre-processing: {df_train.embarked.isnull().sum()}/{len(df_train)}')\n",
    "print(f'Number of null values in Test before pre-processing: {df_test.embarked.isnull().sum()}/{len(df_test)}')\n",
    "\n",
    "#### START CODE HERE (~ 3 line) ####\n",
    "\n",
    "\n",
    "#### END CODE HERE ####\n",
    "\n",
    "print(f'Number of null values in Train after pre-processing: {df_train.embarked.isnull().sum()}/{len(df_train)}')\n",
    "print(f'Number of null values in Test after pre-processing: {df_test.embarked.isnull().sum()}/{len(df_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JshFEcO3OI3Z"
   },
   "source": [
    "### 1.4 Feature selection\n",
    "Feature selection is a critical step in the machine learning pipeline, as it involves choosing the most relevant features (or variables) that contribute to the predictive power of a model. The goal of feature selection is not only to improve the model's performance but also to reduce the computational complexity and enhance the interpretability of the model. The following are the main advantanges produced by an effective feature selection:\n",
    "- **Improves Model Performance**: By removing irrelevant or redundant features, it can increase the accuracy of the model and reduce the risk of overfitting.\n",
    "- **Reduces Training Time**: It can reduce training time by reducing the complexity of the inputs, which is particularly beneficial when dealing with large datasets.\n",
    "- **Increases Model Interpretability**: Models with fewer features are easier to understand and explain, making the results more accessible to non-experts.\n",
    "\n",
    "Identifiers, unique codes, etc., are usually useless features that must be removed.\n",
    "\n",
    "You can learn more about advanced feature selection techniques [here](https://nathanrosidi.medium.com/feature-selection-techniques-in-machine-learning-82c2123bd548)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNjy7LlBObRn"
   },
   "source": [
    "In this exercise, you will just remove features based on the domain knowledge. Specifically, you will remove features that are useless or contain explicit information related to target variable (i.e., the model by using that feature has the information of the actual label). However, data visualization and exploratory data analysis can help in identifying relationships between features and the target variable, as well as spotting redundant features. In this lab, you will also optionally exploit a correlation matrix to remove redundant features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKsJMNNMLA9p"
   },
   "source": [
    "**Exercise**: Remove columns `cabin`, `body`, `boat`, and `home.dest` from the train and test sets because they contain info about the target variable (i.e., the model could \"cheat\" predicting the target label based on the info in these attributes).\n",
    "\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "\n",
    "<ul>\n",
    "    <li> You can find useful the 'df.drop()' method.\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "56k9fHjlOTB-",
    "outputId": "fd07578e-14df-4498-ba2c-0e32ff9ab9af"
   },
   "outputs": [],
   "source": [
    "#### START CODE HERE (~ 2 line) ####\n",
    "\n",
    "\n",
    "#### END CODE HERE ####\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeBjoR04Opez"
   },
   "source": [
    "\n",
    "**Exercise**: Remove other columns that you think are useless features in predicting which people were more likely to survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "PCe20-sVOvMZ",
    "outputId": "bbd8d83a-959b-4a42-bccb-dae7485e32e2"
   },
   "outputs": [],
   "source": [
    "#### START CODE HERE (~ 2 line) ####\n",
    "\n",
    "\n",
    "#### END CODE HERE ####\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hdznv75RNtEa"
   },
   "source": [
    "The next cell plots a **correlation** matrix of the input features with respect to the target variable. The **correlation matrix** is a powerful tool in the data pre-processing phase, especially when you're trying to understand the relationships between your input features and the target variable. Specifically, the **correlation matrix** can be used to:\n",
    "- **Identify Relationships**: It helps in identifying the linear relationship between the input features and the target variable. A high positive or negative correlation indicates a strong relationship, whereas a correlation close to zero suggests no linear relationship.\n",
    "- **Feature Selection**: By analyzing the correlation matrix, you can identify and eliminate features that are highly correlated with each other but not with the target variable. This is because highly correlated features contribute redundant information, which can lead to overfitting.\n",
    "- **Insights for Feature Engineering**: Understanding the relationships between features can also provide insights for feature engineering, such as creating new features that are combinations of existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "2_f5mfEoPMcd",
    "outputId": "e2018a66-f1a5-4f90-9b95-07315832f8ac"
   },
   "outputs": [],
   "source": [
    "df_corr= pd.concat([df_train[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare']], df_train['survived']], axis = 1)\n",
    "\n",
    "g = sns.heatmap(df_corr.corr(),\n",
    "                annot=True,\n",
    "                cmap = \"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40MOLlwyPkEx"
   },
   "source": [
    "**Exercise (optional)**: Remove or combine highly correlated features based on the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Ax6ecNCZPvSv",
    "outputId": "6ea59e8b-990d-450d-a2af-252bd49da4a6"
   },
   "outputs": [],
   "source": [
    "#### START CODE HERE (~ 1 line) ####\n",
    "\n",
    "#### END CODE HERE ####\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YKB0RKEQFMX"
   },
   "source": [
    "### 1.5 Feature engineering\n",
    "\n",
    "Another crucial pre-processing step in the machine learning pipeline is **feature engineering**, which involves creating new features or modifying existing ones to improve the performance of a machine learning model. Spefically, it can be useful to:\n",
    "- **Improve model accuracy**: Effective modified features can capture essential information, making it easier for models to learn.\n",
    "- **Improve model's generalizability**: By capturing the underlying patterns in the data more effectively, feature engineering can help models perform better on unseen data.\n",
    "- **Reduce the need for complex models**: Simpler models with the right features can outperform complex models with a raw set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9qVn82fQIb9"
   },
   "source": [
    "#### Discretization\n",
    "Discretization is a pre-processing step of machine learning that involves transforming continuous features into discrete or categorical ones. This process can be particularly useful for certain models that work better with categorical data, or when looking to simplify the patterns in the data, making them more interpretable for analysis. Discretization can also be beneficial for handling outliers and can improve the performance of some models by creating bins or categories that group continuous data points. The main advantages of using discretization can be summarized in the following:\n",
    "- **Improves model interpretability**: By categorizing continuous features, discretization can potentially make the model's decisions easier to understand.\n",
    "- **Handles outliers**: Outliers can have less impact when the data is divided into bins, as they will fall into the upper or lower bins along with other extreme values.\n",
    "- **Reduces Complexity**: Discretization can act as a form of dimensionality reduction, simplifying the model by reducing the number of unique input values.\n",
    "\n",
    "You can learn more about **discretization** <a href=\"https://trainindata.medium.com/variable-discretization-in-machine-learning-7b09009915c2\" >here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFpjjfF1ucHc"
   },
   "source": [
    "**Exercise**: Discretize the `age` column in the training and test sets into the following categories: `['Child (0-14]', 'Young (14-24]', 'Adults (24-50]', 'Senior (50+]']`. The new discretized age column must by named `age_disc`. The discretized age categories are provided in the `age_category` list. Once performed the discretization, remove the old `age` column from the trining and test set.\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "\n",
    "<ul>\n",
    "    <li> To segment data into bins you can use the `pd.cut` function. Specify the correct bins values and the labels. You should cut on the `age` column. </li>\n",
    "    <li>To remove a column you can use `df.drop()`</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIg8eGklYdVK"
   },
   "outputs": [],
   "source": [
    "age_category = ['Child (0-14]', 'Young (14-24]', 'Adults (24-50]', 'Senior (50+]']\n",
    "\n",
    "#### START CODE HERE (~ 4 line) ####\n",
    "\n",
    "\n",
    "#### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "id59SGUmYuOu",
    "outputId": "dcebb004-591f-472c-ebc5-7b7e2963e793"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujmX35c3QTb5"
   },
   "source": [
    "### 1.8 Feature encoding\n",
    "\n",
    "Machine learning algorithms operate on numerical data, making it essential to convert any categorical input features into a numerical format before training your model. This process, known as **feature encoding**. Proper encoding of input features ensures that the algorithm can interpret the data correctly, leading to more accurate models. Note that this step depends on your algorithm. For instance, Decision trees and their ensembles (e.g., Random Forests) can handle categorical data naturally (depending on the implementation), but many models (such as linear regression, logistic regression, and neural networks) require numerical input.\n",
    "\n",
    "Common encoding techniques include:\n",
    "- **One-Hot Encoding**: For each unique category value, a new binary column is created. A category value is represented by a 1 in its corresponding column and 0s in all others. This method avoids implying an ordinal relationship but increases the feature space. For instance, a variable color containing three possible values, e.g., `red`, `green`, and `blue`, will create three additional columns: `color_red`, `color_green`, and `color_blue`. To represent the color green, you will represent the input with the following vector `[0, 1, 0]`. The main drawback of one-hot encoding is that it can significantly increase the dataset's sparsity (i.e., the number of zeros). Another possible drawback is that, if data has a natural order to the categories (e.g., low, medium, high) one-hot encoding can lose this information (use ordinal encoding in this case).\n",
    "- **Label Encoding**: Each unique category value is assigned an integer value. This method is straightforward but implies an ordinal relationship between categories, which may not always be appropriate.\n",
    "- **Ordinal Encoding**: Similar to label encoding, but the integer assignments are made based on the order specified by the user, making it suitable for ordinal data.\n",
    "\n",
    "You can learn more about all the encoding techniques [here](https://medium.com/anolytics/all-you-need-to-know-about-encoding-techniques-b3a0af68338b).\n",
    "\n",
    "#### One-hot encoding\n",
    "\n",
    "Scikit-Learn make you easy to perform the one-hot encoding with the [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).\n",
    "\n",
    "You can also use a similar approach using Pandas which provide a DataFrame's method `df.get_dummies()` to perform the one-hot encoding ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)).\n",
    "\n",
    "The two approach are similar. The main difference is that the `get_dummies` method does not store the information about train data categories. Hence it may result in inconsistencies with train and test data features. You can learn the differences between `OneHotEncoder` and `get_dummies` [here](https://pythonsimplified.com/difference-between-onehotencoder-and-get_dummies/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpqrldXAw_zr"
   },
   "source": [
    "The next cell performs the one-hot encoding on the **training set** for the `sex` and `embarked` columns using the `OneHotEncoder`. Then, it removes the old columns. The new encoded training set is stored in a new DataFrame `df_train_encoded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agurLVL9ZIiL"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "categorical_columns = ['sex', 'embarked']\n",
    "\n",
    "# Fit the one-hot encoder on training data\n",
    "ohe.fit(df_train[categorical_columns])\n",
    "\n",
    "# Create a new DataFrame with only the one-hot encoded columns\n",
    "temp_df_train = pd.DataFrame(data=ohe.transform(df_train[categorical_columns]).toarray(),\n",
    "                             columns=ohe.get_feature_names_out())\n",
    "\n",
    "# Create a copy of the DataFrame\n",
    "df_train_encoded = df_train.copy()\n",
    "\n",
    "# Remove the old categorical columns from the original data\n",
    "df_train_encoded.drop(columns=categorical_columns, axis=1, inplace=True)\n",
    "df_train_encoded = pd.concat([df_train_encoded.reset_index(drop=True), temp_df_train], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nk-TiycZRPIb"
   },
   "source": [
    "Now, look the differences between the original raw trainig and the encoded training DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ANFVY4asZnkP",
    "outputId": "95db83ea-4924-4b64-977b-240de33a9bf4"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "SjgXOgiQZNAH",
    "outputId": "86135a4f-e985-4104-f21b-34df8fdea72a"
   },
   "outputs": [],
   "source": [
    "df_train_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_A7I5zscxj03"
   },
   "source": [
    "You can see that a new column is created for each distinct category of the encoded columns `sex` and `embarked`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCGREnzZRWra"
   },
   "source": [
    "**Exercise**: Perform the same one-hot encoding on the test set. Create a new DataFrame `df_test_encoded`.\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "\n",
    "<ul>\n",
    "    <li> Remember to do not fit another OneHotEncoder. Instead, use the one fitted on the training set.\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHREfwJ1Z3Tl"
   },
   "outputs": [],
   "source": [
    "#### START CODE HERE (~ 4 lines) ####\n",
    "\n",
    "\n",
    "#### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "pUsW-ckZaFw5",
    "outputId": "583b702b-53f4-4c2b-d70c-c0763cb53514"
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "0TvAOh1naH68",
    "outputId": "79d5b735-9eaa-463e-c79e-ae5a9c673867"
   },
   "outputs": [],
   "source": [
    "df_test_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euTqsMAXQY4G"
   },
   "source": [
    "#### Ordinal encoding\n",
    "\n",
    "With Scikit-Learn you can perform the ordinal encoding with the [OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html).\n",
    "\n",
    "You previously discretized the `age` column into bins, creating a new column `age_disc`. This column must be encoded as well. However, in this case, the categories have an explicit order, Therefore, the ordinal encoding is more suitable.\n",
    "\n",
    "The next cells perform the ordinal encoding of the `age_disc` column on the training set by fitting the OrdinalEncoder on the training data, transform the training dataset column, and delete the old columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "sKoPpONRaUvz",
    "outputId": "19d1eed4-3561-4f45-9933-ce63321a701d"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Instantiate the OrdinalEncoder specifying the list of the categories\n",
    "ord_enc = OrdinalEncoder(categories=[age_category]) # Should be a list becuause you can specify the categories for multiple columns\n",
    "\n",
    "\n",
    "# Fit the OrdinalEncoder on training data\n",
    "ord_enc.fit(df_train_encoded.loc[:, [\"age_disc\"]])\n",
    "\n",
    "ord_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ky497Ns-abKw"
   },
   "outputs": [],
   "source": [
    "# Transform the training data column 'age_disc' into the encoded version 'age_disc_enc'\n",
    "df_train_encoded[\"age_disc_enc\"] = ord_enc.transform(df_train_encoded.loc[:, [\"age_disc\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "acZeHgZUagqr",
    "outputId": "ee5948d2-5a73-4e93-9d15-62211fedb5d0"
   },
   "outputs": [],
   "source": [
    "df_train_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFNqkZE6yrwa"
   },
   "source": [
    "You can see that the new column `age_disc_enc` is represented with an incremental number. Therefore, the order is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "RxCaqqlVamXN",
    "outputId": "6591d91f-b666-4b4c-e9c6-35f918acfb5b"
   },
   "outputs": [],
   "source": [
    "# Delete the old 'age_disc' column\n",
    "df_train_encoded.drop(columns=[\"age_disc\"], axis=1, inplace=True)\n",
    "\n",
    "df_train_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjajFBExTA85"
   },
   "source": [
    "**Exercise**: Perform the same ordinal encoding on the test set, and remove the old `age_disc` column.\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "\n",
    "<ul>\n",
    "    <li> Remember to do not fit another OrdinalEncoder. Instead, use the one fitted on the training set.\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "_oV1h831aq7d",
    "outputId": "fc645e98-5bb6-4052-dd2a-526901c71e5f"
   },
   "outputs": [],
   "source": [
    "#### START CODE HERE (~ 2 lines) ####\n",
    "\n",
    "\n",
    "#### END CODE HERE ####\n",
    "\n",
    "df_test_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzlD2LaFQLz1"
   },
   "source": [
    "### 1.7 Normalization and Standardization\n",
    "\n",
    "\n",
    "Some machine learning algorithms require input features to be **normalized** or **standardized** to work correctly, as this can significantly impact the performance of the model, especially in algorithms that rely on distance computation or gradient descent optimization.\n",
    "\n",
    "**Normalization** and **standardization** are two fundamental pre-processing steps that help to bring the data onto a common scale, making it easier to process by an algorithm. While both methods scale the data, their methods and purposes differ. The choice between normalization and standardization depends on the specific requirements of the algorithm and the nature of the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A **normalization** technique is **Min-Max** normalization. It is a simple tehcnique that rescales the range of features into `[0, 1]`. This is particularly useful when the parameters have to be bounded within a fixed range. It's also useful in algorithms that compute distances between data points and need normalization to ensure that each feature contributes equally to the result.\n",
    "\n",
    "The formula for **Min-Max normalization** is:\n",
    "\n",
    "$$ x\\_norm = \\frac{(x - x_{min})}{(x_{max} - x_{min})} $$\n",
    "\n",
    "Where:\n",
    "- $x$ is the original value.\n",
    "- $x_{min}$ and $x_max$ are the minimum and the maximum of the feature.\n",
    "- $x_{norm}$ is the normilized value.\n",
    "\n",
    "Scikit-Learn provides you an useful class to perform the **Min-Max** normalization, namely [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html).\n",
    "\n",
    "<br><br>\n",
    "\n",
    "A widely used **standardization** technique is the **Z-score** normalization. This method involves rescaling the features so they have the properties of a standard normal distribution with zero mean $\\mu=0$ and standard deviation one $\\sigma = 1$. Standardization is crucial in cases where the data follows a Gaussian distribution and when the algorithm assumes data to be centered around zero.\n",
    "\n",
    "The formula for **Z-score normalization** is:\n",
    "$$ x_{standardized} = \\frac{(x - \\mu)}{\\sigma}$$\n",
    "\n",
    "Where:\n",
    "- $x$ is the original value.\n",
    "- $\\mu$ is the mean of the feature values.\n",
    "- $\\sigma$ is the standard deviation of the feature values.\n",
    "- $ x_{standardized}$ is the standardized value.\n",
    "\n",
    "Unlike Min-Max normalization, standardization does not bind values to a specific range, which makes it useful for features with outliers or many variances. Algorithms like Support Vector Machines, Linear Regression, and Logistic Regression benefit significantly from standardization because it enhances their convergence in optimization algorithms.\n",
    "\n",
    "Scikit-Learn provides you an useful class to perform the **Z-score** normalization, namely [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGlCYWb7X8Hj"
   },
   "source": [
    "**Exercise**: Perform **Min-Max** normalization of the *numerical features* specified in the `numerical_features` variable for both training and test sets. Remember to **fit** on the training and not on the test. Note that `age_disc_enc` in this case is categorical but can be normalized too.\n",
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "\n",
    "<ul>\n",
    "    <li> Use the MinMaxScaler provided by Scikit-Learn.\n",
    "    <li> You must create the MinMaxScaler object, fit on the training encoded DataFrame, and transform the training and test encoded DataFrames.\n",
    "    <li> You must use indexing to correctly access only the numerical features of your DataFrames.\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZI39vQc2YFdw"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "numerical_features = [\"pclass\", \"sibsp\", \"parch\", \"fare\", \"age_disc_enc\"]\n",
    "\n",
    "#### START CODE HERE (~ 4 lines) ####\n",
    "\n",
    "\n",
    "#### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "YNh05P-Ua3rZ",
    "outputId": "fe498802-405a-4ba3-e41e-71a6f2a6e229"
   },
   "outputs": [],
   "source": [
    "df_train_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuqXunjt4Awj"
   },
   "source": [
    "You can see that the numerical features are now rescaled into `[0, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "th8t2yQOa80j",
    "outputId": "f86e3368-7716-4b61-b3b0-95204c1e88b9"
   },
   "outputs": [],
   "source": [
    "df_test_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_BRPKK2Qmmx"
   },
   "source": [
    "### 2.1 Models training and evaluation\n",
    "\n",
    "Scikit-Learn offers a wide range of pre-implemented classification algorithms. You can explore the available Scikit-Learn classification algorithms [here](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning).\n",
    "\n",
    "Training a Scikit-Learn model typically involves the following steps:\n",
    "\n",
    "- **Instantiate the model**: Select the model and create the model object by settings its parameters.\n",
    "- **Training the Model**: Fit your model to the training data using the `.fit()` method.\n",
    "- **Evaluating the Model**: Assess the model's performance on the testing set using metrics such as accuracy, precision, recall, and the confusion matrix. Once the model is trained, you can use the `.predict()` method.\n",
    "- **Parameter Tuning**: Optionally, use cross-validation and grid search to find the best model parameters.\n",
    "\n",
    "You can learn more about Scikit-Learn evaluation metrics [here](https://scikit-learn.org/stable/modules/model_evaluation.html).\n",
    "\n",
    "Scikit-Learn also provides useful functions for [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnY6TGUc4V9H"
   },
   "source": [
    "The next cells train and evaluate a **LogisticRegression** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcMisOGfbvLG"
   },
   "outputs": [],
   "source": [
    "# Extract target variable and input features for the training data\n",
    "y_train = df_train_encoded['survived']               # Target variable trainig set\n",
    "X_train = df_train_encoded.drop('survived', axis=1)  # Features training set\n",
    "\n",
    "\n",
    "# Extract target variable and input features for  the testing data\n",
    "y_test = df_test_encoded['survived']                 # Target variable test set\n",
    "X_test = df_test_encoded.drop('survived', axis=1)    # Features test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "oMid5x2qcEAx",
    "outputId": "473f7b2b-3efc-4238-ca7c-b2dbf8ea97d0"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the model\n",
    "lr_model = LogisticRegression(max_iter=1000)  # Increasing max_iter if convergence warning occurs\n",
    "\n",
    "# Train the model\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Onga49uecKy4",
    "outputId": "6a07bdd2-8c31-48b1-b905-cd9101a18f5a"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Make predictions\n",
    "y_test_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "lr_acc = accuracy_score(y_test, y_test_pred_lr)\n",
    "lr_f1 = f1_score(y_test, y_test_pred_lr, average='macro')\n",
    "\n",
    "# Print accuracy and F1 Score\n",
    "print(f\"Accuracy: {lr_acc:.2f}\")\n",
    "print(f\"F1: {lr_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEO_C7Rg4i_U"
   },
   "source": [
    "Remember that, when the dataset is **imbalanced**, `F1 score` and `recall` are more useful metrics than `accuracy`.\n",
    "\n",
    "Scikit-Learn provides you a useful function to compute several evaluation metrics, namely [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VnhOWtBGrnrk",
    "outputId": "3a70f1f9-5bea-4f8e-f2d4-a38b6ddb14a9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "labels = [\"Not-Survived\", \"Survived\"]\n",
    "\n",
    "classification_report_lr = classification_report(y_test, y_test_pred_lr, target_names=labels)\n",
    "print(classification_report_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUlAZyxV4xgD"
   },
   "source": [
    "The next cell plots the confusion matrix. The confusion matrix is a useful tool for evaluating the performance of classification models.\n",
    "It provides a visual summary of how well the model predicts across different classes, allowing you to see not just the overall accuracy but also more specific details about where the model is making errors.\n",
    "\n",
    "However, in this case the classification task is binary, so the confusion matrix is not very indicative. However, code is given to show how it can be fastly implemented using Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "mzsr5KWDrIZ1",
    "outputId": "8ad43430-8911-4437-f723-3cd71af98f5d"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cmd = ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred_lr, cmap=plt.cm.Blues)\n",
    "ax = cmd.ax_\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uC8_QNDI5rEW"
   },
   "source": [
    "**Exercise**: Train a [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and evaluate its performance. Compute the classification report and store it in a variable `classification_report_rf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lsc6NTpgsStk",
    "outputId": "dfddb38c-d489-45c4-b31b-2520bd99fe46"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#### START CODE HERE (~ 4 lines) ####\n",
    "\n",
    "\n",
    "#### END CODE HERE (~ 4 lines) ####\n",
    "\n",
    "print(classification_report_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYBbwrY-6JIA"
   },
   "source": [
    "The next cells train and evaluate a SupportVectorMachine and a simple Neural Network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "egM9hUH4tj6d",
    "outputId": "abd99215-855c-412a-83a4-fe2c64ee7a8d"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(gamma=1.5, kernel=\"rbf\", probability=True)\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "classification_report_svm = classification_report(y_test, y_test_pred_svm, target_names=labels)\n",
    "print(classification_report_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43Dncbs7uMY-",
    "outputId": "3adc2fea-8492-452b-e50c-12b9f925950e"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(256, 32), max_iter=500).fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_mlp = mlp_model.predict(X_test)\n",
    "\n",
    "classification_report_mlp = classification_report(y_test, y_test_pred_mlp, target_names=labels)\n",
    "print(classification_report_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgQ9IGUuobFl"
   },
   "source": [
    "---\n",
    "\n",
    "## **Exercise 2: Diabetes prediction**\n",
    "\n",
    "In this exercise, you will train machine learning models to predict diabetes in patients based on their medical history and demographic information, using the **Diabetes prediction dataset**.\n",
    "\n",
    "The **Diabetes prediction dataset** is a collection of medical and demographic data records from patients, and their diabetes status (positive or negative).\n",
    "\n",
    "\n",
    "This is an example of real-world medical application. Indeed, this model can be useful for healthcare professionals in identifying patients who may be at risk of developing diabetes and in developing personalized treatment plans.\n",
    "\n",
    "\n",
    "Each record includes several features, such as:\n",
    "- **age**\n",
    "- **gender**\n",
    "- **body mass index** (BMI)\n",
    "- **hypertension**\n",
    "- **heart disease**\n",
    "- **smoking history**\n",
    "- **HbA1c level**\n",
    "- **blood glucose level**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dgi18aT41ZAC"
   },
   "outputs": [],
   "source": [
    "# Import the required libraries for this exercise\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import tree\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wwurSBaBAMk"
   },
   "outputs": [],
   "source": [
    "# If your dataset is stored on Google Drive, mount the drive before reading it\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCHJmpMHVxr9"
   },
   "source": [
    "Before running the next cell, upload the dataset on colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2L9WJXfTFZll",
    "outputId": "a05affb1-e6f6-414b-c205-f22bd7d27ef3"
   },
   "outputs": [],
   "source": [
    "!unzip diabetes-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nkwXEVaBDGt"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/diabetes_prediction_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "VubnWpydrv9T",
    "outputId": "a53526d0-2a3a-478c-82f1-6229dfa7d4c3"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqTQHHKZWThg",
    "outputId": "fc1adbd4-7112-4d87-b752-ca905133cd74"
   },
   "outputs": [],
   "source": [
    "# Check if the dataset is balanced\n",
    "df.diabetes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-q4yoTOJ29W"
   },
   "source": [
    "**Exercise**: Now you will implement the **pre-processing pipeline**, and **train** and **evaluate** a **binary classifier** on the target variable.\n",
    "\n",
    "The following steps are recommended to complete the task. However, it is up to you to make specific choices about the pre-processing to be performed.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Perform the pre-processing:\n",
    "  * Split into **train** and **test** sets (80% train and 20% test).\n",
    "  * **Remove** useless or redundant features.\n",
    "  * **Combine features** to create new features.\n",
    "  * Handling **missing values**.\n",
    "  * Perform **discretization** of features if necessary.\n",
    "  * Encode **categorical features**.\n",
    "  * Perform **normalization** or **standardization** of input features.\n",
    "  * **Encode the target** variable if necessary.\n",
    "\n",
    "\n",
    "\n",
    "2. Train one or more **binary classifiers** to predict the diabetes status of patiens. Use appropriate evaluation metrics to identify the best performing model.\n",
    "\n",
    "***Hints*** :\n",
    "\n",
    ">\n",
    "* When performing the pre-processing steps, compute the statistics on training and transform the test data accordingly.\n",
    "* All the categorical features must be properly encoded.\n",
    "* The dataset is highly imbalanced. F1 score and recall are more appropriate metrics for this task.\n",
    "\n",
    "---\n",
    "\n",
    "This time the exercise is **open-ended**, so it is up to you to write all the code to carry out these steps."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
