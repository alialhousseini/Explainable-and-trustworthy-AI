{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["eOlUbDrl9QNJ","D58lmI-NZP9-"],"authorship_tag":"ABX9TyMOhWTFGMIErRO5SegQtYya"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Lab 1 - Explainable and Trustworthy AI**\n","\n","\n","---\n","\n"],"metadata":{"id":"YO0NhD6Q4BjE"}},{"cell_type":"markdown","source":["**Teaching Assistant**: Eleonora Poeta (eleonora.poeta@polito.it)"],"metadata":{"id":"ZZLb6zMxU0KA"}},{"cell_type":"markdown","source":["**Lab 1:** Interpretable by design models on structured data"],"metadata":{"id":"tmMAwr1FVAXC"}},{"cell_type":"markdown","source":["# **Decision Trees**\n","\n","\n","---\n","\n"],"metadata":{"id":"1Dsi41aUVEUH"}},{"cell_type":"markdown","source":["* Decision trees offer an approach to achieve interpretability-by-design of machine learning models.  They give a transparent and intuitive representation of the decision-making process followed by the model. This transparency allows domain experts to easily understand and validate the model's predictions."],"metadata":{"id":"eiaOmRtTWET3"}},{"cell_type":"markdown","source":["When assessing the interpretability of decision trees, several key aspects should be considered depending if assessing *global* or *local* interpretability.\n","In particular, you have to analyze:\n","\n","\n","\n","1.   When assessing **global** interpretability you have to *inspect the entire decision tree*. Then, as mesures for the global interpretability there are:\n","\n","\n","> * **Depth** of the tree → Shallow trees with fewer levels are easier to interpret, as they represent simpler decision rules. In contrast, deeper trees may become overly complex and difficult to interpret, potentially sacrificing transparency for improved accuracy.\n","* **Size** of the tree → This includes the *number of nodes* and the *number of splits*. A larger tree with more nodes and splits may capture intricate patterns in the data but could also lead to overfitting and decreased interpretability.\n","\n","\n","\n","\n","2.  When assessing **local** interpretability you have to *inspect the individual path of a single prediction.* Then, as mesures for the local interpretability there is:\n","\n","\n","> * **Lenght** of the individual path.\n"],"metadata":{"id":"Qol6kdvZXFf0"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","## **Exercise 1**\n","The *Diabetes prediction dataset* comprises medical and demographic data, alongside diabetes status (positive:1/negative:0) of patients. It includes features like age, gender, body mass index (BMI), hypertension, heart disease, smoking history, HbA1c level, and blood glucose level.\n","In the following exercise you have to:\n","\n","* Fit a [**Decision tree classifier**](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) model on **Diabetes dataset** and evaluate it calculating model's accuracy.\n","\n","\n","> *   Visualize the decision tree obtained. Are you able to interpret the decision process?\n","*   Try again with `max_depth=4` and compare the two trees. Which one is the most interpretable?\n","\n","\n","\n","\n","* Analyze **Global Interpretability**:\n","\n","\n","> * Continue visualizing the obtained decision tree with `max_depth=4`. Which attributes are the most discriminating? Plot the feature importances and then analyze the values.\n","* Calculate the size of the decision tree in terms of the number of nodes, subdivisions, and depth. How these metrics affect the interpretability of the decision tree globally?\n","\n","\n","* Analyze **Local Interpretability**:\n","\n","\n","> * Consider the instances 100, 150 and 200 of the train dataset.\n","* What are the individual paths? What are the instances allocated in the paths?\n","* For each of the previous instances, calculate the length of each path from the root node to the leaf node to which the instance belongs. How the length of these paths contributes to the interpretability of the decision tree locally?\n","\n","\n","***Hint*** :\n","\n","> * Before starting do some **preprocessing** of the Diabetes dataset as previously seen in Lab 0.1 (Address null values and preprocess categorical attributes.)\n","* As **split ratio** for the dataset use the standard one: train (80%) and test (20%). Account for any class imbalance during the train-test split by making use of the **stratify** argument\n","\n"],"metadata":{"id":"WhUj0BpZdxn4"}},{"cell_type":"markdown","source":["## **Solution:**"],"metadata":{"id":"qO25v5G1fnXU"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jNFArpWBvlAt","outputId":"a7205c50-2b11-4b52-c408-1c5c9f961db4","executionInfo":{"status":"ok","timestamp":1711301960154,"user_tz":-60,"elapsed":21822,"user":{"displayName":"Eleonora Poeta","userId":"04524499908072715947"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# If your dataset is stored on Google Drive, mount the drive before reading it\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["### Data Preprocessing"],"metadata":{"id":"xyJ9S2oIfIh9"}},{"cell_type":"code","source":["### Write your solution here"],"metadata":{"id":"qaQC6Dhuv2ik"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Fit the decision tree"],"metadata":{"id":"s6qqXI00i1az"}},{"cell_type":"code","source":["### Write your solution here"],"metadata":{"id":"s6y6iPpWwBSC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Decision Tree: Max-depth = 4"],"metadata":{"id":"eDO0Bb1IxIs_"}},{"cell_type":"code","source":["### Write your solution here"],"metadata":{"id":"QEkmSvGTwD_v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Analyze Global Interpretability"],"metadata":{"id":"_4J9AJkYkQma"}},{"cell_type":"code","source":["### Write your solution here"],"metadata":{"id":"DcOaH7oiwGPF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Analyze Local Interpretability"],"metadata":{"id":"nkwu_yEHkUMB"}},{"cell_type":"code","source":["### Write your solution here"],"metadata":{"id":"m1aUiS1NwHWT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Logistic Regression**\n","\n","\n","---\n","\n"],"metadata":{"id":"D6TkB65PXkug"}},{"cell_type":"markdown","source":["* Logistic regression is utilized to estimate probabilities in classification scenarios featuring two potential outcomes.\n","* Unlike linear regression, the interpretation of logistic regression weights varies since the outcome in logistic regression is a probability confined within the range of 0 to 1. These weights undergo transformation through the logistic function, influencing the probability non-linearly."],"metadata":{"id":"CQrLtZuG0DrG"}},{"cell_type":"markdown","source":["When assessing interpretability in Logistic Regression models you have to inspect:\n","\n","1.   **Coefficient Magnitudes (Weights)** → Examine the magnitude and sign of coefficients to understand the impact of each predictor variable on the log-odds of the outcome. Larger magnitude coefficients suggest a stronger influence on the outcome.\n","2. **Odds Ratios** → Calculate and interpret odds ratios for each predictor variable. Odds ratios provide a clear understanding of how the odds of the outcome change with a one-unit increase in the predictor variable.\n","\n","The interpretation of features depends on the feature types:\n","\n","\n","\n","* **Numerical features** : If you increase the value of the analyzed feature by one unit, the estimated odds change by a factor of exp(βj).\n","* **Binary Categorical features** : Changing the feature from the reference category to the other category changes the estimated odds change by a factor of exp(βj).\n"],"metadata":{"id":"vg2XCCjA2bOY"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","## **Exercise 2**\n","\n","\n","\n","*   Fit a [**Logistic Regression**](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) model on the same dataset as before.\n","\n","* Compute the Correlation Matrix.\n","\n","* Evaluate the model using Precision, Recall and F1-score metrics.\n","\n","* Analyze **Interpretability**:\n","\n","\n","> * Visualize the estimated **weights** and **odds ratios** obtained from logistic regression for each feature.\n","* Put them into a tabular form and interpret the logistic regression model for different types of features. Specifically, analyze the ***bmi*** feature and ***gender_Male*** and ***gender_Female*** features.\n","\n","\n","\n","\n","***Hint:***\n","\n","\n","> For displaying metrics use the **classification_report** function from scikit-learn.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"IMkm58NP3RPb"}},{"cell_type":"markdown","source":["## **Solution:**"],"metadata":{"id":"eOlUbDrl9QNJ"}},{"cell_type":"markdown","source":["### Fit the Logistic Regression model and evaluate it"],"metadata":{"id":"h2wOyuxYwOgu"}},{"cell_type":"code","source":["### Write your solution here"],"metadata":{"id":"t1zmvV0KwUTL","executionInfo":{"status":"ok","timestamp":1711305484485,"user_tz":-60,"elapsed":4,"user":{"displayName":"Eleonora Poeta","userId":"04524499908072715947"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["### Analyze Interpretability"],"metadata":{"id":"5NSxxtVpwq4T"}},{"cell_type":"code","source":["### Write your solution here"],"metadata":{"id":"SxmARUxKwu-d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **KNN**\n","\n","\n","---\n","\n"],"metadata":{"id":"AS9rfOb6XTLf"}},{"cell_type":"markdown","source":["* In KNN, the prediction for a new data point is determined by the majority class (for classification) or the mean of the closest k neighbors' values (for regression) among the training data points, where *k* is a user-defined parameter.\n","\n","* Finding *k* can be tricky and you typically use techniques like cross-validation, grid search, or random search."],"metadata":{"id":"AKlF4cR4lTx-"}},{"cell_type":"markdown","source":["The interpretation and explanation of KNN does not follow procedures similar to the ones above. This because the KNN is an **instance-based learning algorithm** and so the **interpretation** can be done ***by-example***.\n","\n","To explain a prediction in KNN:\n","\n","\n","\n","1.   Retrieve the *k neighbors* that were used for the prediction.\n","2.   Analyze the k neighbors.\n","\n","Hence, KNN offers a **local explanation** of the prediction as we explain the prediction of an individual instance by its closest example.\n","\n"],"metadata":{"id":"J7Q_HfpMW7dT"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","## **Exercise 3**\n","\n","\n","* Search for a **resonable** ***k*** parameter iterating over a range (1, 15) using a [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n","\n","* Use the **best k** already found and fit a [**KNN**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) on the same dataset as before.\n","\n","* Evaluate the KNN calculating the **mean accuracy** over test dataset.\n","\n","* Analyze **Interpretability**:\n","\n","\n","> * Consider the instances 100, 150 and 200 of the train dataset.\n","* **Check** the **predicted target** for each instance.\n","* **Retrieve** the relative k-nearest neighbors.\n","* To **explain** the **predicted target** check the predicted targets for each of the k-nearest neighbors.\n","\n","\n","\n","***Hint:***\n","\n","\n","> For calculating the mean accuracy you can use the [score( )](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.score) function from scikit-learn.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"T_DnK6lCZb0a"}},{"cell_type":"markdown","source":["## **Solution:**"],"metadata":{"id":"713m7jq8ZK9Y"}},{"cell_type":"markdown","source":["### Search for a reasonable ***K*** parameter and fit the KNN classifier"],"metadata":{"id":"s8mVL9EhccTj"}},{"cell_type":"code","source":["### Write your solution here"],"metadata":{"id":"8szqjodvwadv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Analyze interpretability"],"metadata":{"id":"I0svxFGeekzr"}},{"cell_type":"markdown","source":["\n","\n","---\n","Instance 100\n"],"metadata":{"id":"cy_x1PbowgiG"}},{"cell_type":"code","source":["### Write your solution here"],"metadata":{"id":"bRrMfABswfp8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","Instance 150"],"metadata":{"id":"8_Pcqw9DvHVb"}},{"cell_type":"code","source":["### Write your solution here"],"metadata":{"id":"K4hqPtNhwkF1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","Instance 200"],"metadata":{"id":"7YKeau2Iva4D"}},{"cell_type":"code","source":["### Write your solution here"],"metadata":{"id":"F4P6cVQtwlZW"},"execution_count":null,"outputs":[]}]}